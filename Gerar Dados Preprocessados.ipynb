{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a298dfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas carregadas com sucesso\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np # biblioteca usada para trabalhar com vetores e matrizes\n",
    "import pandas as pd # biblioteca usada para trabalhar com dataframes e análise de dados\n",
    "import sklearn as skl # sckit-learn\n",
    "import time # biblioteca usada para calcular o tempo de execucao\n",
    "import re # biblioteca para expressoes regulares\n",
    "import os # biblioteca usada para realizar tarefas específicas ao SO\n",
    "\n",
    "\n",
    "# importa alguns pacotes do sckit-learn\n",
    "from sklearn import model_selection \n",
    "from sklearn import naive_bayes # necessario para usar o metodo naive Bayes\n",
    "from sklearn import model_selection # necessario para fazer validacao cruzada\n",
    "from sklearn import metrics # necessario para obter o desempenho da classificacao\n",
    "\n",
    "import nltk # biblioteca para Processamento de Linguagem Natural\n",
    "from nltk.stem.porter import PorterStemmer # para fazer a estemização em documentos da lingua inglesa\n",
    "from nltk.stem import RSLPStemmer # para fazer a estemização em documentos da lingua portuguesa\n",
    "\n",
    "import unicodedata # sera usada para remover acentos dos documentos em lingua portuguesa\n",
    "\n",
    "# bibliotecas para geração de gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# biblioteca para tratar com caminho de arquivo\n",
    "from pathlib import Path\n",
    "\n",
    "# biblioteca para leitura de csv\n",
    "import csv\n",
    "\n",
    "# biblioteca para acessar diretórios e arquivos\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "print('Bibliotecas carregadas com sucesso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1214ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(globals()['_dh'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ebca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "499c86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConverterMesParaNumero(texto):\n",
    "    texto = texto.replace('janeiro', '1')\n",
    "    texto = texto.replace('fevereiro', '2')\n",
    "    texto = texto.replace('março', '3')\n",
    "    texto = texto.replace('abril', '4')\n",
    "    texto = texto.replace('maio', '5')\n",
    "    texto = texto.replace('junho', '6')\n",
    "    texto = texto.replace('julho', '7')\n",
    "    texto = texto.replace('agosto', '8')\n",
    "    texto = texto.replace('setembro', '9')\n",
    "    texto = texto.replace('outubro', '10')\n",
    "    texto = texto.replace('novembro', '11')\n",
    "    texto = texto.replace('dezembro', '12')\n",
    "    return texto\n",
    "\n",
    "def ObterData(texto):\n",
    "    data = re.findall(r'\\d+', ConverterMesParaNumero(texto))[:3]\n",
    "    if '-' in texto:\n",
    "        data.reverse()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f657c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportDatasetToArray(pathDataset):\n",
    "    dataset = []\n",
    "    \n",
    "    with open(pathDataset, newline='', encoding='utf-8-sig') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        next(reader, None)\n",
    "        \n",
    "        for row in reader:\n",
    "            dataset.append(row)\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d60c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportDatasetToArrayFakeBr(dirDataset, dirMetaDataset, tipo):\n",
    "    dataset = []\n",
    "    \n",
    "    os.chdir(dirDataset)\n",
    "    for i, file in zip(range(len(os.listdir())), os.listdir()):\n",
    "        row = []\n",
    "        file = open(file, 'r', encoding='utf-8-sig')\n",
    "        row.append(i)\n",
    "        row.append(file.read())\n",
    "        dataset.append(row)\n",
    "        \n",
    "    os.chdir(dirMetaDataset)\n",
    "    for i, file in zip(range(len(os.listdir())), os.listdir()):\n",
    "        file = open(file, 'r', encoding='utf-8-sig')\n",
    "        lines = file.readlines()\n",
    "        data = ObterData(lines[3])\n",
    "        dataset[i] = dataset[i] + data + [lines[1]]\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7df2b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, stemming = False, stopwords = False):\n",
    "    \"\"\"\n",
    "    Funcao usada para fazer o tratamento de textos da lingua portuguesa\n",
    "    \n",
    "    Parametros: \n",
    "        text: variavel do tipo string que contem o texto que devera ser tratado\n",
    "        \n",
    "        stemming: variavel do tipo booleana que indica se a estemizacao deve ser aplicada ou nao\n",
    "        \n",
    "        stopwords: variavel do tipo booleana que indica se as stopwords devem ser removidas ou nao\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove tags HTML\n",
    "    regex = re.compile('<[^<>]+>')\n",
    "    text = re.sub(regex, \" \", text) \n",
    "    \n",
    "    # normaliza as URLs\n",
    "    regex = re.compile('(http|https)://[^\\s]*')\n",
    "    text = re.sub(regex, \"<URL>\", text)\n",
    "\n",
    "    # normaliza emails\n",
    "    regex = re.compile('[^\\s]+@[^\\s]+')\n",
    "    text = re.sub(regex, \"<EMAIL>\", text)\n",
    "\n",
    "    #normaliza o símbolo de dólar\n",
    "    regex = re.compile('[$]+')\n",
    "    text = re.sub(regex, \"<MONEY>\", text)    \n",
    "    \n",
    "    # converte todos os caracteres não-alfanuméricos em espaço\n",
    "    regex = re.compile('[^A-Za-z0-9]+')  \n",
    "    text = re.sub(regex, \" \", text)\n",
    "    \n",
    "    # normaliza os numeros \n",
    "    regex = re.compile('[0-9]+')\n",
    "    text = re.sub(regex, \"<NUMBER>\", text)\n",
    "    \n",
    "    # substitui varios espaçamentos seguidos em um só\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # remove stopwords\n",
    "    if stopwords:\n",
    "        words = text.split() # separa o texto em palavras\n",
    "        words = [w for w in words if not w in nltk.corpus.stopwords.words('portuguese')]\n",
    "        text = \" \".join( words )\n",
    "    \n",
    "    # aplica estemização\n",
    "    if stemming: \n",
    "        stemmer_method = RSLPStemmer()  \n",
    "        words = text.split() # separa o texto em palavras\n",
    "        words = [ stemmer_method.stem(w) for w in words ]\n",
    "        text = \" \".join( words )\n",
    "    \n",
    "    # remove palavras de apenas um caracter\n",
    "    words = text.split() # separa o texto em palavras\n",
    "    words = [ w for w in words if len(w)>1 ]\n",
    "    text = \" \".join( words )\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a07bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTrue = ImportDatasetToArray('./input/data/true.csv')\n",
    "datasetFake = ImportDatasetToArray('./input/data/fake.csv')\n",
    "\n",
    "datasetTrue = np.asarray(datasetTrue)\n",
    "datasetFake = np.asarray(datasetFake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bf4e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1630 de 1630\r"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate(datasetTrue):\n",
    "    datasetTrue[i][1] = preprocessing(datasetTrue[i][1], stemming = True, stopwords = True)\n",
    "    \n",
    "    if i%50==0 or i==len(datasetTrue)-1:\n",
    "        print(' %d de %d' %(i+1, len(datasetTrue)), end='\\r')\n",
    "        \n",
    "for i, msg in enumerate(datasetFake):\n",
    "    datasetFake[i][1] = preprocessing(datasetFake[i][1], stemming = True, stopwords = True)\n",
    "    \n",
    "    if i%50==0 or i==len(datasetFake)-1:\n",
    "        print(' %d de %d' %(i+1, len(datasetFake)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f69bcc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(datasetTrue)\n",
    "dataset.to_csv('./output/preprocessed/basetcc/true.csv', sep=';', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab3edfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(datasetFake)\n",
    "dataset.to_csv('./output/preprocessed/basetcc/fake.csv', sep=';', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5e1e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "truePath, trueMetaPath = os.path.abspath('./input/data/true'), os.path.abspath('./input/data/true-meta-information')\n",
    "fakePath, fakeMetaPath = os.path.abspath('./input/data/fake'), os.path.abspath('./input/data/fake-meta-information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b872df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTrueBr = ImportDatasetToArrayFakeBr(truePath, trueMetaPath, 'true')\n",
    "datasetFakeBr = ImportDatasetToArrayFakeBr(fakePath, fakeMetaPath, 'fake')\n",
    "\n",
    "datasetTrueBr = np.asarray(datasetTrueBr)\n",
    "datasetFakeBr = np.asarray(datasetFakeBr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90995c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3551 de 3600\r"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate(datasetTrueBr):\n",
    "    datasetTrueBr[i][1] = preprocessing(datasetTrueBr[i][1], stemming = True, stopwords = True)\n",
    "    \n",
    "    if i%50==0 or i==len(datasetTrueBr)-1:\n",
    "        print(' %d de %d' %(i+1, len(datasetTrueBr)), end='\\r')\n",
    "        \n",
    "for i, msg in enumerate(datasetFakeBr):\n",
    "    datasetFakeBr[i][1] = preprocessing(datasetFakeBr[i][1], stemming = True, stopwords = True)\n",
    "    \n",
    "    if i%50==0 or i==len(datasetFake)-1:\n",
    "        print(' %d de %d' %(i+1, len(datasetFakeBr)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3fca9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(globals()['_dh'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f233ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(datasetTrueBr)\n",
    "dataset.to_csv('./output/preprocessed/fakebr/true.csv', sep=';', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21e784fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(datasetFakeBr)\n",
    "dataset.to_csv('./output/preprocessed/fakebr/fake.csv', sep=';', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8491bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trueNormalized = os.path.abspath('./input/data/size_normalized_texts/true')\n",
    "fakeNormalized = os.path.abspath('./input/data/size_normalized_texts/fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ea49c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTrueNormalized = ImportDatasetToArrayFakeBr(trueNormalized, trueMetaPath, 'true')\n",
    "datasetFakeNormalized = ImportDatasetToArrayFakeBr(fakeNormalized, fakeMetaPath, 'fake')\n",
    "\n",
    "datasetTrueNormalized = np.asarray(datasetTrueNormalized)\n",
    "datasetFakeNormalized = np.asarray(datasetFakeNormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "617872c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3600 de 3600\r"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate(datasetTrueNormalized):\n",
    "    datasetTrueNormalized[i][1] = preprocessing(datasetTrueNormalized[i][1], stemming = True, stopwords = True)\n",
    "    \n",
    "    if i%50==0 or i==len(datasetTrueNormalized)-1:\n",
    "        print(' %d de %d' %(i+1, len(datasetTrueNormalized)), end='\\r')\n",
    "        \n",
    "for i, msg in enumerate(datasetFakeNormalized):\n",
    "    datasetFakeNormalized[i][1] = preprocessing(datasetFakeNormalized[i][1], stemming = True, stopwords = True)\n",
    "    \n",
    "    if i%50==0 or i==len(datasetFakeNormalized)-1:\n",
    "        print(' %d de %d' %(i+1, len(datasetFakeNormalized)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5c49c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(globals()['_dh'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db8a5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(datasetTrueNormalized)\n",
    "dataset.to_csv('./output/preprocessed/normfakebr/true.csv', sep=';', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b17a4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(datasetFakeNormalized)\n",
    "dataset.to_csv('./output/preprocessed/normfakebr/fake.csv', sep=';', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6317fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trueTruncado = os.path.abspath('./input/data/textos_truncados/true.csv')\n",
    "fakeTruncado = os.path.abspath('./input/data/textos_truncados/fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48584a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetTrueTruncado = ImportDatasetToArray(trueTruncado)\n",
    "datasetFakeTruncado = ImportDatasetToArray(fakeTruncado)\n",
    "\n",
    "datasetTrueTruncado = np.asarray(datasetTrueTruncado)\n",
    "datasetFakeTruncado = np.asarray(datasetFakeTruncado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9302dca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1630 de 1630\r"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate(datasetTrueTruncado):\n",
    "    datasetTrueTruncado[i][1] = preprocessing(datasetTrueTruncado[i][1], stemming = True, stopwords = True)\n",
    "    \n",
    "    if i%50==0 or i==len(datasetTrueTruncado)-1:\n",
    "        print(' %d de %d' %(i+1, len(datasetTrueTruncado)), end='\\r')\n",
    "        \n",
    "for i, msg in enumerate(datasetFakeTruncado):\n",
    "    datasetFakeTruncado[i][1] = preprocessing(datasetFakeTruncado[i][1], stemming = True, stopwords = True)\n",
    "    \n",
    "    if i%50==0 or i==len(datasetFakeTruncado)-1:\n",
    "        print(' %d de %d' %(i+1, len(datasetFakeTruncado)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47375704",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(globals()['_dh'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03a8f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(datasetTrueTruncado)\n",
    "dataset.to_csv('./output/preprocessed/normtcc/true.csv', sep=';', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36ffafc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(datasetFakeTruncado)\n",
    "dataset.to_csv('./output/preprocessed/normtcc/fake.csv', sep=';', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac68d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
